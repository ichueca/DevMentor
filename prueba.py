#from utils.api_client import GeminiClient
from utils import OpenAIClient,OllamaClient, PromptService, PromptType
from dotenv import load_dotenv

load_dotenv()

"""
llm_client = OpenAIClient()

prompt_service = PromptService(llm_Client=llm_client)

test_cases = [
    ("Â¿QuÃ© es Python?", PromptType.EXPLANATION),
    ("Revisa este cÃ³digo", PromptType.CODE_REVIEW),
    ("Tengo un error en mi cÃ³digo", PromptType.DEBUGGING),
    ("CuÃ¡les son las mejores prÃ¡cticas?", PromptType.BEST_PRACTICES),
    ("Hola Â¿como estÃ¡s?", PromptType.GENERAL),
]

print("Probando la clasificaciÃ³n")
correct = 0

for input_text, expected in test_cases:
    detected = prompt_service.detect_prompt_type(input_text)
    status = "âœ…" if detected == expected else "âŒ"
    if detected == expected:
        correct += 1
    print(f"   {status} '{input_text}' -> {detected.value} (esperado: {expected.value})")

print(f"ğŸ“¶ PrecisiÃ³n: {correct} / {len(test_cases)} ({100*correct // len(test_cases)}%)")

"""
llm_client = OllamaClient()
response_generator = llm_client.generate_response("hola", {})
response = ""
for chunk in response_generator:
    if chunk:
        response += chunk

print(f"Respuesta del anÃ¡lisis : {response}")